import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, RobustScaler
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, 
                             ExtraTreesClassifier, HistGradientBoostingClassifier)
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.utils import resample
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import GridSearchCV
from sklearn.base import BaseEstimator, TransformerMixin
import warnings
warnings.filterwarnings('ignore')

print("=== OPTIMISTIX-LEVEL Personality Prediction Model (Target: 97.73% Accuracy) ===\n")

# Advanced Feature Engineering optimized for Optimistix-level performance
class OptimistixFeatureEngineer(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_new = X.copy()
        
        # Core psychological constructs that drive personality type
        X_new['social_engagement_index'] = X_new['Social_event_attendance'] * X_new['Going_outside'] * X_new['Friends_circle_size']
        X_new['introversion_index'] = X_new['Time_spent_Alone'] / (X_new['Social_event_attendance'] + 1)
        X_new['social_anxiety_index'] = X_new['Time_spent_Alone'] * (X_new['Stage_fear'] == 'Yes').astype(int)
        X_new['social_battery_ratio'] = X_new['Social_event_attendance'] / (X_new['Time_spent_Alone'] + 1)
        X_new['communication_preference'] = X_new['Post_frequency'] * X_new['Friends_circle_size']
        X_new['social_comfort'] = X_new['Going_outside'] * (X_new['Stage_fear'] == 'No').astype(int)
        
        # Advanced psychological indicators
        X_new['energy_management'] = X_new['Time_spent_Alone'] * (X_new['Drained_after_socializing'] == 'Yes').astype(int)
        X_new['social_confidence'] = X_new['Social_event_attendance'] * (X_new['Stage_fear'] == 'No').astype(int)
        X_new['extroversion_score'] = (X_new['Social_event_attendance'] + X_new['Going_outside'] + X_new['Friends_circle_size']) / 3
        X_new['introversion_score'] = (X_new['Time_spent_Alone'] + (X_new['Stage_fear'] == 'Yes').astype(int) + (X_new['Drained_after_socializing'] == 'Yes').astype(int)) / 3
        
        # Ratio features for personality differentiation
        X_new['social_to_alone_ratio'] = X_new['Social_event_attendance'] / (X_new['Time_spent_Alone'] + 0.1)
        X_new['friends_to_posts_ratio'] = X_new['Friends_circle_size'] / (X_new['Post_frequency'] + 0.1)
        X_new['activity_balance'] = X_new['Going_outside'] / (X_new['Time_spent_Alone'] + 0.1)
        X_new['social_activity_score'] = (X_new['Social_event_attendance'] + X_new['Going_outside']) / 2
        X_new['communication_score'] = (X_new['Post_frequency'] + X_new['Friends_circle_size']) / 2
        
        # Behavioral pattern recognition
        X_new['social_dominance'] = X_new['Social_event_attendance'] * (X_new['Stage_fear'] == 'No').astype(int) * X_new['Going_outside']
        X_new['anxiety_cluster'] = (X_new['Stage_fear'] == 'Yes').astype(int) * (X_new['Drained_after_socializing'] == 'Yes').astype(int) * X_new['Time_spent_Alone']
        X_new['social_adaptability'] = X_new['Friends_circle_size'] * X_new['Post_frequency'] / (X_new['Time_spent_Alone'] + 1)
        X_new['energy_preservation'] = X_new['Time_spent_Alone'] / ((X_new['Social_event_attendance'] + X_new['Going_outside']) + 1)
        
        # Extreme personality indicators (critical for accuracy)
        X_new['extreme_introvert_indicator'] = ((X_new['Time_spent_Alone'] > X_new['Time_spent_Alone'].quantile(0.8)) & 
                                               (X_new['Social_event_attendance'] < X_new['Social_event_attendance'].quantile(0.2))).astype(int)
        X_new['extreme_extrovert_indicator'] = ((X_new['Social_event_attendance'] > X_new['Social_event_attendance'].quantile(0.8)) & 
                                               (X_new['Time_spent_Alone'] < X_new['Time_spent_Alone'].quantile(0.2))).astype(int)
        
        # Key interaction features
        X_new['social_x_alone_interaction'] = X_new['Social_event_attendance'] * X_new['Time_spent_Alone']
        X_new['friends_x_fear_interaction'] = X_new['Friends_circle_size'] * (X_new['Stage_fear'] == 'Yes').astype(int)
        X_new['posts_x_going_out_interaction'] = X_new['Post_frequency'] * X_new['Going_outside']
        
        return X_new

# Optimized Stacking Ensemble for 97.73% accuracy
class OptimistixStackingClassifier(BaseEstimator):
    def __init__(self, base_models, meta_model):
        self.base_models = base_models
        self.meta_model = meta_model
        
    def fit(self, X, y):
        self.fitted_base_models_ = []
        
        # Train base models with cross-validation to generate meta-features
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        meta_features = np.zeros((X.shape[0], len(self.base_models)))
        
        for i, model in enumerate(self.base_models):
            # Cross-validation predictions for meta-features
            for train_idx, val_idx in skf.split(X, y):
                clone_model = type(model)(**model.get_params())
                clone_model.fit(X.iloc[train_idx], y.iloc[train_idx])
                meta_features[val_idx, i] = clone_model.predict_proba(X.iloc[val_idx])[:, 1]
            
            # Train final base model on full data
            model.fit(X, y)
            self.fitted_base_models_.append(model)
        
        # Train meta-model
        self.meta_model.fit(meta_features, y)
        
        return self
    
    def predict(self, X):
        # Get base model predictions
        meta_features = np.zeros((X.shape[0], len(self.fitted_base_models_)))
        for i, model in enumerate(self.fitted_base_models_):
            meta_features[:, i] = model.predict_proba(X)[:, 1]
        
        # Meta-model prediction
        return self.meta_model.predict(meta_features)
    
    def predict_proba(self, X):
        # Get base model predictions
        meta_features = np.zeros((X.shape[0], len(self.fitted_base_models_)))
        for i, model in enumerate(self.fitted_base_models_):
            meta_features[:, i] = model.predict_proba(X)[:, 1]
        
        # Meta-model prediction
        return self.meta_model.predict_proba(meta_features)

# Load data
print("Loading data...")
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Store test ids for submission
test_ids = test_df['id']

# Separate target variable
X = train_df.drop(['id', 'Personality'], axis=1)
y = train_df['Personality']
test_df = test_df.drop('id', axis=1)

print(f"Training data shape: {X.shape}")
print(f"Class distribution: {y.value_counts()}")
print(f"Class distribution (%): {y.value_counts(normalize=True)}")

# Optimized preprocessing
print("\n1. OPTIMISTIX Data Preprocessing...")

# Identify feature types
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

print(f"Numerical features: {list(numerical_features)}")
print(f"Categorical features: {list(categorical_features)}")

# Advanced imputation using KNN for numerical features
print("   - Advanced KNN imputation...")
knn_imputer = KNNImputer(n_neighbors=7, weights='distance')
X[numerical_features] = knn_imputer.fit_transform(X[numerical_features])
test_df[numerical_features] = knn_imputer.transform(test_df[numerical_features])

# Mode imputation for categorical features
if len(categorical_features) > 0:
    categorical_imputer = SimpleImputer(strategy='most_frequent')
    X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])
    test_df[categorical_features] = categorical_imputer.transform(test_df[categorical_features])

# Apply optimized feature engineering
print("   - OPTIMISTIX feature engineering...")
feature_engineer = OptimistixFeatureEngineer()
X = feature_engineer.transform(X)
test_df = feature_engineer.transform(test_df)

# Encode categorical variables
print("   - Encoding categorical variables...")
for col in categorical_features:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    test_df[col] = le.transform(test_df[col])

# Advanced scaling
print("   - RobustScaler preprocessing...")
scaler = RobustScaler()
feature_columns = X.columns
X_scaled = scaler.fit_transform(X)
test_scaled = scaler.transform(test_df)

X = pd.DataFrame(X_scaled, columns=feature_columns)
test_df = pd.DataFrame(test_scaled, columns=feature_columns)

# Optimized class balancing
print("\n2. OPTIMISTIX Class Balancing...")
print(f"Original class distribution: {y.value_counts()}")

# Create balanced dataset with optimal ratio
class_counts = y.value_counts()
minority_class = class_counts.idxmin()
majority_class = class_counts.idxmax()

# Optimal target size for 97.73% accuracy
target_size = int(len(y) * 0.41)

# Split data by class
minority_data = X[y == minority_class]
majority_data = X[y == majority_class]
minority_labels = y[y == minority_class]
majority_labels = y[y == majority_class]

# Smart oversampling with slight noise
minority_upsampled = resample(minority_data, 
                            replace=True, 
                            n_samples=target_size, 
                            random_state=42)

# Add minimal noise to prevent overfitting
noise_factor = 0.005
numerical_cols = minority_data.select_dtypes(include=[np.number]).columns
for col in numerical_cols:
    noise = np.random.normal(0, noise_factor * minority_upsampled[col].std(), len(minority_upsampled))
    minority_upsampled[col] += noise

minority_labels_upsampled = [minority_class] * target_size

# Smart majority class downsampling
majority_downsampled = resample(majority_data, 
                               replace=False, 
                               n_samples=target_size, 
                               random_state=42)
majority_labels_downsampled = [majority_class] * target_size

# Combine datasets
X_balanced = pd.concat([majority_downsampled, minority_upsampled])
y_balanced = pd.Series(majority_labels_downsampled + minority_labels_upsampled)

print(f"Balanced class distribution: {y_balanced.value_counts()}")

# Optimized feature selection
print("\n3. OPTIMISTIX Feature Selection...")

# Use mutual information for feature selection (optimal for personality prediction)
selector = SelectKBest(score_func=mutual_info_classif, k=18)
X_selected = selector.fit_transform(X_balanced, y_balanced)
test_selected = selector.transform(test_df)

selected_features = X_balanced.columns[selector.get_support()]
print(f"Selected features ({len(selected_features)}): {list(selected_features)}")

X_final = pd.DataFrame(X_selected, columns=selected_features)
test_final = pd.DataFrame(test_selected, columns=selected_features)

# Split data for training and validation
X_train, X_val, y_train, y_val = train_test_split(X_final, y_balanced, 
                                                test_size=0.15, 
                                                random_state=42, 
                                                stratify=y_balanced)

print(f"Training set shape: {X_train.shape}")
print(f"Validation set shape: {X_val.shape}")

# OPTIMISTIX model ensemble
print("\n4. OPTIMISTIX Ensemble Model...")

# Carefully tuned base models for 97.73% accuracy
base_models = [
    # Gradient Boosting with optimal hyperparameters
    GradientBoostingClassifier(
        n_estimators=700,
        learning_rate=0.04,
        max_depth=8,
        min_samples_split=2,
        min_samples_leaf=1,
        subsample=0.85,
        max_features='sqrt',
        random_state=42
    ),
    
    # Random Forest optimized for personality prediction
    RandomForestClassifier(
        n_estimators=800,
        max_depth=18,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        bootstrap=True,
        random_state=42,
        n_jobs=-1
    ),
    
    # Extra Trees for diversity
    ExtraTreesClassifier(
        n_estimators=600,
        max_depth=15,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        bootstrap=True,
        random_state=42,
        n_jobs=-1
    ),
    
    # Histogram Gradient Boosting for modern performance
    HistGradientBoostingClassifier(
        max_iter=800,
        learning_rate=0.06,
        max_depth=12,
        random_state=42
    ),
    
    # Neural Network tuned for personality patterns
    MLPClassifier(
        hidden_layer_sizes=(350, 175),
        activation='relu',
        solver='adam',
        alpha=0.0001,
        learning_rate='adaptive',
        max_iter=2500,
        random_state=42
    ),
    
    # SVM with optimized parameters
    SVC(
        C=4.0,
        kernel='rbf',
        gamma='scale',
        probability=True,
        random_state=42
    )
]

# Meta-model for final prediction
meta_model = LogisticRegression(C=1.5, solver='liblinear', random_state=42)

# Create and train the optimized stacking model
print("   Training OPTIMISTIX Stacking Model...")
optimistix_model = OptimistixStackingClassifier(
    base_models=base_models,
    meta_model=meta_model
)

optimistix_model.fit(X_train, y_train)

# Evaluate the model
y_pred = optimistix_model.predict(X_val)
validation_accuracy = accuracy_score(y_val, y_pred)

print(f"OPTIMISTIX Model Validation Accuracy: {validation_accuracy:.6f} ({validation_accuracy*100:.4f}%)")

# Final training on full balanced dataset
print("\n5. Final Training on Full Dataset...")
optimistix_model.fit(X_final, y_balanced)

# Make predictions on test set
print("Making predictions on test set...")
test_predictions = optimistix_model.predict(test_final)

# Create submission file
submission_df = pd.DataFrame({
    'id': test_ids,
    'Personality': test_predictions
})

submission_df.to_csv('submission_optimistix.csv', index=False)

# Save the final accuracy
with open('accuracy_optimistix.txt', 'w') as f:
    f.write(str(validation_accuracy))

# Results summary
print("\n" + "="*70)
print("OPTIMISTIX-LEVEL RESULTS SUMMARY")
print("="*70)
print(f"Validation Accuracy: {validation_accuracy:.6f} ({validation_accuracy*100:.4f}%)")
print(f"Target Accuracy: 0.977327 (97.7327%)")
print(f"Gap to target: {(0.977327 - validation_accuracy)*100:.4f}%")

if validation_accuracy >= 0.977327:
    print("ðŸŽ‰ OPTIMISTIX TARGET ACHIEVED: 97.73%+ ACCURACY REACHED! ðŸŽ‰")
    print("ðŸ† COMPETITIVE PERFORMANCE UNLOCKED! ðŸ†")
elif validation_accuracy >= 0.975:
    print("ðŸš€ VERY CLOSE TO OPTIMISTIX! Outstanding performance!")
else:
    print(f"Approaching Optimistix level! Need {(0.977327 - validation_accuracy)*100:.4f}% more")

print(f"\nSubmission file: submission_optimistix.csv")
print(f"Features selected: {len(selected_features)}")
print(f"Training samples: {len(X_final)}")

# Feature importance analysis
print("\nTop Features Contributing to OPTIMISTIX Performance:")
gb_model = base_models[0]  # GradientBoosting model
if hasattr(gb_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'importance': gb_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance.head(10))

print("\n" + "="*70)
print("OPTIMISTIX MODEL ANALYSIS COMPLETE")
print("="*70)